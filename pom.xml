<?xml version="1.0" encoding="UTF-8"?>
<!--
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at
  http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
		 xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">

	<modelVersion>4.0.0</modelVersion>

		<groupId>org.apache.flink</groupId>
		<version>1.15-SNAPSHOT</version>

	<artifactId>flink-sql-client</artifactId>
	<name>Flink : Table : SQL Client</name>
	<description>
		This module contains the SQL Client for exploring and
		submitting SQL programs to Flink.
	</description>

	<properties>
		<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>
		<project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>
		<flink.version>1.15-SNAPSHOT</flink.version>
		<flink.shaded.jackson.version>2.12.4</flink.shaded.jackson.version>
		<flink.shaded.version>14.0</flink.shaded.version>
		<hadoop.version>2.4.1</hadoop.version>
		<hamcrest.version>1.3</hamcrest.version>
		<hive.version>2.3.4</hive.version>
		<hivemetastore.hadoop.version>2.7.5</hivemetastore.hadoop.version>
		<java.version>1.8</java.version>
		<log4j.version>2.17.0</log4j.version>
		<maven.compiler.source>${java.version}</maven.compiler.source>
		<maven.compiler.target>${java.version}</maven.compiler.target>
		<scala.binary.version>2.12</scala.binary.version>

		<flink.forkCount>1C</flink.forkCount>
		<flink.reuseForks>true</flink.reuseForks>
		<flink.forkCountTestPackage>${flink.forkCount}</flink.forkCountTestPackage>
		<test.randomization.seed/>
		<test.unit.pattern>**/*Test.*</test.unit.pattern>
	</properties>
	<packaging>jar</packaging>

	<dependencies>

		<!-- core dependencies -->

		<!-- Flink modules -->
		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-core</artifactId>
			<version>${flink.version}</version>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-clients</artifactId>
			<version>${flink.version}</version>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-streaming-scala_${scala.binary.version}</artifactId>
			<version>${flink.version}</version>
		</dependency>

		<!-- Table ecosystem -->
		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-table-common</artifactId>
			<version>${flink.version}</version>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-table-api-java</artifactId>
			<version>${flink.version}</version>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-table-api-java-bridge</artifactId>
			<version>${flink.version}</version>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-table-planner_${scala.binary.version}</artifactId>
			<version>${flink.version}</version>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-table-runtime</artifactId>
			<version>${flink.version}</version>
		</dependency>

		<!-- CLI for SQL CLI client -->
		<dependency>
			<groupId>org.jline</groupId>
			<artifactId>jline-terminal</artifactId>
			<version>3.21.0</version>
		</dependency>

		<dependency>
			<groupId>org.jline</groupId>
			<artifactId>jline-reader</artifactId>
			<version>3.21.0</version>
		</dependency>

		<!-- configuration -->
		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-shaded-jackson</artifactId>
			<version>${flink.shaded.jackson.version}-${flink.shaded.version}</version>
		</dependency>

		<!-- test dependencies -->

		<dependency>
			<groupId>org.hamcrest</groupId>
			<artifactId>hamcrest-all</artifactId>
			<version>${hamcrest.version}</version>
			<type>jar</type>
			<scope>test</scope>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-streaming-java</artifactId>
			<version>${flink.version}</version>
			<type>test-jar</type>
			<scope>test</scope>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-csv</artifactId>
			<version>${flink.version}</version>
			<scope>test</scope>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-runtime</artifactId>
			<version>${flink.version}</version>
			<type>test-jar</type>
			<scope>test</scope>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-test-utils</artifactId>
			<version>${flink.version}</version>
			<scope>test</scope>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-clients</artifactId>
			<version>${flink.version}</version>
			<type>test-jar</type>
			<scope>test</scope>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-connector-hive_${scala.binary.version}</artifactId>
			<version>${flink.version}</version>
			<scope>provided</scope>
			<optional>true</optional>
			<exclusions>
				<exclusion>
					<groupId>org.apache.hadoop</groupId>
					<artifactId>hadoop-hdfs</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-python_${scala.binary.version}</artifactId>
			<version>${flink.version}</version>
			<scope>test</scope>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-table-api-scala-bridge_${scala.binary.version}</artifactId>
			<version>${flink.version}</version>
			<scope>test</scope>
		</dependency>
		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-table-planner_${scala.binary.version}</artifactId>
			<version>${flink.version}</version>
			<type>test-jar</type>
			<scope>test</scope>
		</dependency>

		<!-- test dependencies for HiveCatalog. The exclusions should be in sync with those flink-connector-hive -->

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-connector-hive_${scala.binary.version}</artifactId>
			<version>${flink.version}</version>
			<type>test-jar</type>
			<scope>test</scope>
		</dependency>

		<dependency>
			<groupId>org.apache.flink</groupId>
			<artifactId>flink-hadoop-compatibility_${scala.binary.version}</artifactId>
			<version>${flink.version}</version>
			<scope>test</scope>
		</dependency>

		<dependency>
			<groupId>org.apache.hive</groupId>
			<artifactId>hive-metastore</artifactId>
			<version>${hive.version}</version>
			<scope>test</scope>
			<exclusions>
				<exclusion>
					<groupId>org.apache.hive</groupId>
					<artifactId>hive-shims</artifactId>
				</exclusion>
				<exclusion>
					<groupId>javolution</groupId>
					<artifactId>javolution</artifactId>
				</exclusion>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
				<exclusion>
					<groupId>com.google.protobuf</groupId>
					<artifactId>protobuf-java</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.hbase</groupId>
					<artifactId>hbase-client</artifactId>
				</exclusion>
				<exclusion>
					<groupId>commons-lang</groupId>
					<artifactId>commons-lang</artifactId>
				</exclusion>
				<exclusion>
					<groupId>com.zaxxer</groupId>
					<artifactId>HikariCP</artifactId>
				</exclusion>
				<exclusion>
					<groupId>co.cask.tephra</groupId>
					<artifactId>tephra-api</artifactId>
				</exclusion>
				<exclusion>
					<groupId>co.cask.tephra</groupId>
					<artifactId>tephra-core</artifactId>
				</exclusion>
				<exclusion>
					<groupId>co.cask.tephra</groupId>
					<artifactId>tephra-hbase-compat-1.0</artifactId>
				</exclusion>
				<exclusion>
					<groupId>commons-cli</groupId>
					<artifactId>commons-cli</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.thrift</groupId>
					<artifactId>libfb303</artifactId>
				</exclusion>
				<exclusion>
					<groupId>javax.transaction</groupId>
					<artifactId>transaction-api</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.orc</groupId>
					<artifactId>orc-core</artifactId>
				</exclusion>
				<exclusion>
					<groupId>joda-time</groupId>
					<artifactId>joda-time</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.logging.log4j</groupId>
					<artifactId>log4j-1.2-api</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.logging.log4j</groupId>
					<artifactId>log4j-slf4j-impl</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.ant</groupId>
					<artifactId>ant</artifactId>
				</exclusion>
				<exclusion>
					<groupId>com.tdunning</groupId>
					<artifactId>json</artifactId>
				</exclusion>
				<exclusion>
					<groupId>jline</groupId>
					<artifactId>jline</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.eclipse.jetty.aggregate</groupId>
					<artifactId>jetty-all</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.eclipse.jetty.orbit</groupId>
					<artifactId>javax.servlet</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.logging.log4j</groupId>
					<artifactId>log4j-web</artifactId>
				</exclusion>
				<exclusion>
					<groupId>io.dropwizard.metrics</groupId>
					<artifactId>metrics-core</artifactId>
				</exclusion>
				<exclusion>
					<groupId>io.dropwizard.metrics</groupId>
					<artifactId>metrics-jvm</artifactId>
				</exclusion>
				<exclusion>
					<groupId>io.dropwizard.metrics</groupId>
					<artifactId>metrics-json</artifactId>
				</exclusion>
				<exclusion>
					<groupId>com.github.joshelser</groupId>
					<artifactId>dropwizard-metrics-hadoop-metrics2-reporter</artifactId>
				</exclusion>

				<!-- org.apache.hive:hive-service-rpc -->
				<exclusion>
					<groupId>tomcat</groupId>
					<artifactId>jasper-compiler</artifactId>
				</exclusion>
				<exclusion>
					<groupId>tomcat</groupId>
					<artifactId>jasper-runtime</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.httpcomponents</groupId>
					<artifactId>httpclient</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.httpcomponents</groupId>
					<artifactId>httpcore</artifactId>
				</exclusion>

				<!-- org.apache.hive:hive-serde -->
				<exclusion>
					<groupId>commons-codec</groupId>
					<artifactId>commons-codec</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.avro</groupId>
					<artifactId>avro</artifactId>
				</exclusion>
				<exclusion>
					<groupId>net.sf.opencsv</groupId>
					<artifactId>opencsv</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.parquet</groupId>
					<artifactId>parquet-hadoop-bundle</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
			<groupId>org.apache.hive</groupId>
			<artifactId>hive-exec</artifactId>
			<version>${hive.version}</version>
			<scope>test</scope>
			<exclusions>
				<exclusion>
					<groupId>org.apache.hive</groupId>
					<artifactId>hive-vector-code-gen</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.hive</groupId>
					<artifactId>hive-llap-tez</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.hive</groupId>
					<artifactId>hive-shims</artifactId>
				</exclusion>
				<exclusion>
					<groupId>commons-codec</groupId>
					<artifactId>commons-codec</artifactId>
				</exclusion>
				<exclusion>
					<groupId>commons-httpclient</groupId>
					<artifactId>commons-httpclient</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.logging.log4j</groupId>
					<artifactId>log4j-1.2-api</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.logging.log4j</groupId>
					<artifactId>log4j-slf4j-impl</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.antlr</groupId>
					<artifactId>antlr-runtime</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.antlr</groupId>
					<artifactId>ST4</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.ant</groupId>
					<artifactId>ant</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.commons</groupId>
					<artifactId>commons-compress</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.ivy</groupId>
					<artifactId>ivy</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.zookeeper</groupId>
					<artifactId>zookeeper</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.curator</groupId>
					<artifactId>apache-curator</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.curator</groupId>
					<artifactId>curator-framework</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.codehaus.groovy</groupId>
					<artifactId>groovy-all</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.calcite</groupId>
					<artifactId>calcite-core</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.calcite</groupId>
					<artifactId>calcite-druid</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.calcite.avatica</groupId>
					<artifactId>avatica</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.calcite</groupId>
					<artifactId>calcite-avatica</artifactId>
				</exclusion>
				<exclusion>
					<groupId>com.google.code.gson</groupId>
					<artifactId>gson</artifactId>
				</exclusion>
				<exclusion>
					<groupId>stax</groupId>
					<artifactId>stax-api</artifactId>
				</exclusion>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
			<groupId>org.apache.hadoop</groupId>
			<artifactId>hadoop-common</artifactId>
			<version>${hivemetastore.hadoop.version}</version>
			<scope>test</scope>
			<exclusions>
				<exclusion>
					<groupId>com.google.guava</groupId>
					<artifactId>guava</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.curator</groupId>
					<artifactId>curator-client</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.curator</groupId>
					<artifactId>curator-recipes</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.avro</groupId>
					<artifactId>avro</artifactId>
				</exclusion>
				<exclusion>
					<groupId>commons-digester</groupId>
					<artifactId>commons-digester</artifactId>
				</exclusion>
				<exclusion>
					<groupId>com.sun.jersey</groupId>
					<artifactId>jersey-json</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.codehaus.jackson</groupId>
					<artifactId>jackson-mapper-asl</artifactId>
				</exclusion>
				<exclusion>
					<groupId>log4j</groupId>
					<artifactId>log4j</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.slf4j</groupId>
					<artifactId>slf4j-log4j12</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
			<groupId>org.apache.hadoop</groupId>
			<artifactId>hadoop-mapreduce-client-core</artifactId>
			<version>${hivemetastore.hadoop.version}</version>
			<scope>provided</scope>
			<exclusions>
				<exclusion>
					<groupId>org.apache.curator</groupId>
					<artifactId>curator-test</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.hadoop</groupId>
					<artifactId>hadoop-yarn-common</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.apache.avro</groupId>
					<artifactId>avro</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.codehaus.jackson</groupId>
					<artifactId>jackson-core-asl</artifactId>
				</exclusion>
				<exclusion>
					<groupId>org.slf4j</groupId>
					<artifactId>slf4j-log4j12</artifactId>
				</exclusion>
			</exclusions>
		</dependency>

		<dependency>
			<!-- Replaces hadoop-mapreduce-client-core log4j dependency -->
			<groupId>org.apache.logging.log4j</groupId>
			<artifactId>log4j-1.2-api</artifactId>
			<version>${log4j.version}</version>
			<scope>provided</scope>
		</dependency>

	</dependencies>

	<build>
		<plugins>
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-surefire-plugin</artifactId>
				<version>3.0.0-M5</version>
				<configuration>
					<!-- enables TCP/IP communication between surefire and forked JVM-->
					<forkNode implementation="org.apache.maven.plugin.surefire.extensions.SurefireForkNodeFactory"/>
					<forkCount>${flink.forkCount}</forkCount>
					<reuseForks>${flink.reuseForks}</reuseForks>
					<trimStackTrace>false</trimStackTrace>
					<systemPropertyVariables>
						<forkNumber>0${surefire.forkNumber}</forkNumber>
						<hadoop.version>${hadoop.version}</hadoop.version>
						<checkpointing.randomization>true</checkpointing.randomization>
						<buffer-debloat.randomization>true</buffer-debloat.randomization>
						<!-- force the use of the Changelog State Backend in tests on mini-cluster
							on: enable CheckpointingOptions.ENABLE_STATE_CHANGE_LOG on cluster level
							random: enable it randomly, unless explicitly set
							unset: don't alter the configuration
						-->
						<!-- Changelog backend is disabled until FLINK-25399 is resolved -->
						<checkpointing.changelog>unset</checkpointing.changelog>
						<project.basedir>${project.basedir}</project.basedir>
						<!--suppress MavenModelInspection -->
						<test.randomization.seed>${test.randomization.seed}</test.randomization.seed>
					</systemPropertyVariables>
					<argLine>-Xms256m -Xmx2048m -Dmvn.forkNumber=${surefire.forkNumber} -XX:+UseG1GC</argLine>
				</configuration>
				<executions>
					<!--execute all the unit tests-->
					<execution>
						<id>default-test</id>
						<phase>test</phase>
						<goals>
							<goal>test</goal>
						</goals>
						<configuration>
							<includes>
								<include>${test.unit.pattern}</include>
							</includes>
						</configuration>
					</execution>
					<!--execute all the integration tests-->
					<execution>
						<id>integration-tests</id>
						<phase>integration-test</phase>
						<goals>
							<goal>test</goal>
						</goals>
						<configuration>
							<includes>
								<include>**/*.*</include>
							</includes>
							<excludes>
								<exclude>${test.unit.pattern}</exclude>
								<!-- Exclude classes generated by Scala that surefire rejects
								     e.g., 'org.apache.flink.api.scala.typeutils.Foo$Bar$Foobar'. -->
								<exclude>**/*$*</exclude>
							</excludes>
							<reuseForks>false</reuseForks>
						</configuration>
					</execution>
				</executions>
			</plugin>
			<!-- Build flink-sql-client jar -->
			<plugin>
				<groupId>org.apache.maven.plugins</groupId>
				<artifactId>maven-shade-plugin</artifactId>
				<version>3.1.1</version>
				<executions>
					<!-- Exclude all flink-dist files -->
					<execution>
						<id>shade-flink</id>
						<phase>package</phase>
						<goals>
							<goal>shade</goal>
						</goals>
						<configuration>
							<artifactSet>
								<includes combine.children="append">
									<include>org.jline:*</include>
								</includes>
							</artifactSet>
						</configuration>
					</execution>
				</executions>
			</plugin>
		</plugins>
	</build>
</project>
